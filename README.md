# AI Research Paper Assistant

This repository contains a small Python application that loads a research paper (PDF), splits it into logical sections, builds a FAISS vector index over sectionised chunks, and exposes a Streamlit UI for searching and browsing the document with a Retrieval-Augmented Generation (RAG) pipeline.

## ðŸ“ Project Structure

```
AI_ResearchPaper_Assistant/
â”œâ”€â”€ Faiss_index.py        # builds FAISS index with section metadata
â”œâ”€â”€ parser.py             # PDF loader and section splitter
â”œâ”€â”€ rag_engine.py         # retrieval + LLM answer generator
â”œâ”€â”€ main.py               # simple CLI wrapper (optional)
â”œâ”€â”€ app.py                # Streamlit user interface
â”œâ”€â”€ env/                  # Python virtual environment (not committed)
â”œâ”€â”€ README.md             # this file
â””â”€â”€ TrafficFlowGAN_.pdf   # example paper (removed by default)
```

## ðŸ› ï¸ Requirements

- Python 3.10 (or later)
- `venv` or other virtual environment manager

### Python dependencies

The following packages are used (install via `pip`):

```
pip install streamlit langchain langchain-classic langchain-community langchain-openai langchain-huggingface langchain-heck at*  # etc
```

**Note:** The repository originally used `langchain_classic` and `langchain_community`, make sure these modules are available. You may also need FAISS (`faiss-cpu`) and any embedding or transformer packages used in `Faiss_index.py`.

Using the included `env` directory is one fast way to ensure the correct environment; activate it with:

```bash
source env/bin/activate
```

## ðŸš€ Running the Application

1. **Activate the environment** (if not already active):
   ```bash
   source env/bin/activate
   ```

2. **Install any missing dependencies** (if not using the pre-built env):
   ```bash
   pip install -r requirements.txt
   ```
   *You may need to create this file yourself based on imports.*

3. **Start the Streamlit UI**:
   ```bash
   ./env/bin/streamlit run app.py
   ```

   or

   ```bash
   streamlit run app.py
   ```

   Make sure Streamlit is invoked from the same Python environment where the packages were installed.

4. **Upload a PDF research paper** using the sidebar uploader.
   - The app will split the document into sections, chunk each section, build/update the FAISS index, and display navigation options.
   - Enter natural language queries to ask the model about the paper. Responses are generated via the RAG pipeline.

5. **Browse sections** using the "Browse Sections" tab. Content is displayed in a scrollable text area.

6. **View paper overview** statistics in the "Paper Overview" tab.

## ðŸ§© Key Components

### `parser.py`
- Uses `PyMuPDFLoader` to extract text from each page of a PDF.
- Implements `split_into_sections()` to detect numbered and keyword headings and return `(heading, content)` pairs.

### `Faiss_index.py`
- Loads text via `parser.load_pdf` and `split_into_sections`.
- Splits sections into chunks using `RecursiveCharacterTextSplitter`.
- Builds an in-memory FAISS index using `HuggingFaceEmbeddings` with optional caching and section metadata.

### `rag_engine.py`
- Configures a retriever from the FAISS vector store and a cross-encoder reranker for better relevance.
- Constructs a prompt template and runs an LLM (`ChatOpenAI` with a Groq API endpoint in this case) to answer queries based on retrieved chunks.

### `app.py`
- Streamlit front end providing:
  - PDF upload capability
  - Dynamic navigation based on upload
  - Search interface with answer display and source chunk visibility
  - Section browsing and overview statistics
- Uses caching to avoid rebuilding the index unnecessarily and clears cache when a new file is uploaded.

### `main.py`
- A simple command-line wrapper for quick testing of `RAG_Engine` if a paper is already loaded.

## âš ï¸ Notes & Tips

- **Caching behavior**: `get_vectorstore_and_sections` is cached with `@st.cache_resource`. When uploading a new file, the cache is explicitly cleared to ensure the index rebuilds.

- **Environment issues**: Always run Streamlit and Python commands from the same environment (`env` directory) to avoid `ModuleNotFoundError` errors.

- **Customization**: You can modify the section-heading regex in `parser.py` or adjust chunk size/overlap in `Faiss_index.py` to suit different document styles.

- **Dependencies**: For reproducibility, consider generating a `requirements.txt` via `pip freeze > requirements.txt`.

## ðŸ§ª Testing the App

- Upload a known PDF such as `TrafficFlowGAN_.pdf` to verify sections are detected correctly.
- Ask questions like "What is TrafficFlowGAN?" to see RAG responses.

## ðŸ“¦ Distribution

This project is designed for local use and exploration. If you wish to package it:

1. Ensure all dependencies are listed.
2. Optionally containerize with Docker for portability.
3. Provide instructions for setting up API keys (e.g., Groq or OpenAI) in `.env`.

---

Happy experimenting with your AI research paper assistant! Let me know if you'd like help adding more features or deploying the app.
